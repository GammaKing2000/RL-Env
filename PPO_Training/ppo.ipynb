{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c2e329e3",
   "metadata": {},
   "source": [
    "# Training Grow-R Environment with PPO"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24f4cb19",
   "metadata": {},
   "source": [
    "## Importing Required Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "75a42104",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\pygame\\pkgdata.py:25: UserWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html. The pkg_resources package is slated for removal as early as 2025-11-30. Refrain from using this package or pin to Setuptools<81.\n",
      "  from pkg_resources import resource_stream, resource_exists\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import sys\n",
    "import gymnasium as gym\n",
    "\n",
    "# Add the parent directory to the path to allow for package imports\n",
    "notebook_dir = os.getcwd()\n",
    "sys.path.insert(0, os.path.abspath(os.path.join(notebook_dir, '..')))\n",
    "\n",
    "from sb3_contrib import RecurrentPPO\n",
    "from stable_baselines3.common.env_util import make_vec_env\n",
    "from stable_baselines3.common.callbacks import EvalCallback, StopTrainingOnRewardThreshold\n",
    "from stable_baselines3.common.evaluation import evaluate_policy\n",
    "from stable_baselines3.common.vec_env import VecNormalize\n",
    "from stable_baselines3.common import logger\n",
    "from collections import OrderedDict\n",
    "from training_utils import SaveOnIntervalCallback, visualise_training_logs\n",
    "from plantos_env import PlantOSEnv"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc08f94a",
   "metadata": {},
   "source": [
    "## Define paths for saving models and logs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2060e000",
   "metadata": {},
   "outputs": [],
   "source": [
    "training_run = \"10M\"\n",
    "\n",
    "MODEL_DIR = os.path.join(\"PPO_Training/models\", training_run)\n",
    "LOG_DIR = os.path.join(\"PPO_Training/logs\", training_run)\n",
    "TENSORBOARD_LOG_DIR = \"PPO_Training/logs\"\n",
    "os.makedirs(MODEL_DIR, exist_ok=True)\n",
    "os.makedirs(LOG_DIR, exist_ok=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "31be3c59",
   "metadata": {},
   "source": [
    "## Setting PPO Hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "505f6190",
   "metadata": {},
   "outputs": [],
   "source": [
    "# PPO Hyperparameters\n",
    "config = OrderedDict([('batch_size', 64),\n",
    "            ('clip_range', 0.18),\n",
    "            ('ent_coef', 0.0),\n",
    "            ('gae_lambda', 0.95),\n",
    "            ('gamma', 0.999),\n",
    "            ('learning_rate', 0.0003),\n",
    "            ('n_epochs', 10),\n",
    "            ('n_steps', 2048),\n",
    "            ('n_timesteps', 1000000.0),\n",
    "            ('normalize', True),\n",
    "            ('policy', 'MlpLstmPolicy'),\n",
    "            ('policy_kwargs', dict(net_arch=[256, 256])),\n",
    "            ('normalize_kwargs', {'norm_obs': True, 'norm_reward': False})])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "83532e86",
   "metadata": {},
   "source": [
    "## Initialising the Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "74d3efb7",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\gymnasium\\envs\\registration.py:788: UserWarning: \u001b[33mWARN: The environment is being initialised with render_mode='rgb_array' that is not in the possible render_modes ([]).\u001b[0m\n",
      "  logger.warn(\n"
     ]
    }
   ],
   "source": [
    "n_envs = 4\n",
    "env_kwargs = {\n",
    "    'grid_size': 21,\n",
    "    'num_plants': 20,\n",
    "    'num_obstacles': 12,\n",
    "    'lidar_range': 6,\n",
    "    'lidar_channels': 32,\n",
    "    'observation_mode': 'grid',\n",
    "    'thirsty_plant_prob': 0.5\n",
    "}\n",
    "\n",
    "# Create vectorized environment using the registered environment ID\n",
    "env = make_vec_env('PlantOS-v0', n_envs=n_envs, env_kwargs=env_kwargs)\n",
    "if config['normalize']:\n",
    "    env = VecNormalize(env, norm_obs=config['normalize_kwargs']['norm_obs'], norm_reward=config['normalize_kwargs']['norm_reward'], clip_obs=10.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c05afbe9",
   "metadata": {},
   "source": [
    "## Initialising the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "bb6347d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using cpu device\n",
      "Logging to PPO_Training/logs\\10M\n"
     ]
    }
   ],
   "source": [
    "model = RecurrentPPO(\n",
    "        config['policy'],\n",
    "        env,\n",
    "        device='cpu',\n",
    "        verbose=1,\n",
    "        learning_rate=float(config['learning_rate']),\n",
    "        batch_size=int(config['batch_size']),\n",
    "        gamma=float(config['gamma']),\n",
    "        clip_range=config['clip_range'],\n",
    "        ent_coef=config['ent_coef'],\n",
    "        gae_lambda=config['gae_lambda'],\n",
    "        n_epochs=config['n_epochs'],\n",
    "        n_steps=config['n_steps'],\n",
    "        policy_kwargs=config['policy_kwargs'],\n",
    "        #tensorboard_log=TENSORBOARD_LOG_DIR\n",
    "    )\n",
    "\n",
    "# Log the training\n",
    "new_logger = logger.configure(LOG_DIR, [\"stdout\", \"csv\"])\n",
    "model.set_logger(new_logger)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0f2922c",
   "metadata": {},
   "source": [
    "## Setting up Callbacks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8223fe8f",
   "metadata": {},
   "outputs": [],
   "source": [
    "save_interval = 100000\n",
    "save_callback = SaveOnIntervalCallback(save_interval, MODEL_DIR)\n",
    "combined_callbacks = [save_callback]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "750c2e14",
   "metadata": {},
   "source": [
    "## Training the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "3acdfaa5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting PPO training with Stable Baselines3...\n",
      "---------------------------------\n",
      "| rollout/           |          |\n",
      "|    ep_len_mean     | 40.1     |\n",
      "|    ep_rew_mean     | 149      |\n",
      "| time/              |          |\n",
      "|    fps             | 2149     |\n",
      "|    iterations      | 1        |\n",
      "|    time_elapsed    | 3        |\n",
      "|    total_timesteps | 8192     |\n",
      "---------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 61          |\n",
      "|    ep_rew_mean          | 228         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 146         |\n",
      "|    iterations           | 2           |\n",
      "|    time_elapsed         | 111         |\n",
      "|    total_timesteps      | 16384       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.010650488 |\n",
      "|    clip_fraction        | 0.171       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.6        |\n",
      "|    explained_variance   | -0.00243    |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 60.4        |\n",
      "|    n_updates            | 10          |\n",
      "|    policy_gradient_loss | -0.0189     |\n",
      "|    value_loss           | 1.46e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 90.3        |\n",
      "|    ep_rew_mean          | 332         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 118         |\n",
      "|    iterations           | 3           |\n",
      "|    time_elapsed         | 207         |\n",
      "|    total_timesteps      | 24576       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.014195323 |\n",
      "|    clip_fraction        | 0.216       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.58       |\n",
      "|    explained_variance   | 0.156       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 486         |\n",
      "|    n_updates            | 20          |\n",
      "|    policy_gradient_loss | -0.00807    |\n",
      "|    value_loss           | 1.64e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 137         |\n",
      "|    ep_rew_mean          | 415         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 114         |\n",
      "|    iterations           | 4           |\n",
      "|    time_elapsed         | 286         |\n",
      "|    total_timesteps      | 32768       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.008302516 |\n",
      "|    clip_fraction        | 0.134       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.258       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 459         |\n",
      "|    n_updates            | 30          |\n",
      "|    policy_gradient_loss | -0.00858    |\n",
      "|    value_loss           | 2.09e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 192         |\n",
      "|    ep_rew_mean          | 545         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 120         |\n",
      "|    iterations           | 5           |\n",
      "|    time_elapsed         | 340         |\n",
      "|    total_timesteps      | 40960       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009540042 |\n",
      "|    clip_fraction        | 0.144       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.56       |\n",
      "|    explained_variance   | 0.0974      |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 206         |\n",
      "|    n_updates            | 40          |\n",
      "|    policy_gradient_loss | -0.00919    |\n",
      "|    value_loss           | 1.48e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 257         |\n",
      "|    ep_rew_mean          | 689         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 124         |\n",
      "|    iterations           | 6           |\n",
      "|    time_elapsed         | 393         |\n",
      "|    total_timesteps      | 49152       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009137724 |\n",
      "|    clip_fraction        | 0.139       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.53       |\n",
      "|    explained_variance   | 0.322       |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 301         |\n",
      "|    n_updates            | 50          |\n",
      "|    policy_gradient_loss | -0.00748    |\n",
      "|    value_loss           | 2.06e+03    |\n",
      "-----------------------------------------\n",
      "-----------------------------------------\n",
      "| rollout/                |             |\n",
      "|    ep_len_mean          | 275         |\n",
      "|    ep_rew_mean          | 756         |\n",
      "| time/                   |             |\n",
      "|    fps                  | 128         |\n",
      "|    iterations           | 7           |\n",
      "|    time_elapsed         | 444         |\n",
      "|    total_timesteps      | 57344       |\n",
      "| train/                  |             |\n",
      "|    approx_kl            | 0.009383688 |\n",
      "|    clip_fraction        | 0.101       |\n",
      "|    clip_range           | 0.18        |\n",
      "|    entropy_loss         | -1.52       |\n",
      "|    explained_variance   | 0.34        |\n",
      "|    learning_rate        | 0.0003      |\n",
      "|    loss                 | 68.7        |\n",
      "|    n_updates            | 60          |\n",
      "|    policy_gradient_loss | -0.00867    |\n",
      "|    value_loss           | 2.13e+03    |\n",
      "-----------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mKeyboardInterrupt\u001b[39m                         Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[7]\u001b[39m\u001b[32m, line 2\u001b[39m\n\u001b[32m      1\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mStarting PPO training with Stable Baselines3...\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m----> \u001b[39m\u001b[32m2\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m      3\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mconfig\u001b[49m\u001b[43m[\u001b[49m\u001b[33;43m'\u001b[39;49m\u001b[33;43mn_timesteps\u001b[39;49m\u001b[33;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;66;43;03m# 10 Million\u001b[39;49;00m\n\u001b[32m      4\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcombined_callbacks\u001b[49m\n\u001b[32m      5\u001b[39m \u001b[43m)\u001b[49m\n\u001b[32m      6\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33m\"\u001b[39m\u001b[33mPPO Training Finished.\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m      7\u001b[39m \u001b[38;5;28mprint\u001b[39m(\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mTotal timesteps trained: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmodel.num_timesteps\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m\"\u001b[39m)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:450\u001b[39m, in \u001b[36mRecurrentPPO.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    441\u001b[39m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[34mlearn\u001b[39m(\n\u001b[32m    442\u001b[39m     \u001b[38;5;28mself\u001b[39m: SelfRecurrentPPO,\n\u001b[32m    443\u001b[39m     total_timesteps: \u001b[38;5;28mint\u001b[39m,\n\u001b[32m   (...)\u001b[39m\u001b[32m    448\u001b[39m     progress_bar: \u001b[38;5;28mbool\u001b[39m = \u001b[38;5;28;01mFalse\u001b[39;00m,\n\u001b[32m    449\u001b[39m ) -> SelfRecurrentPPO:\n\u001b[32m--> \u001b[39m\u001b[32m450\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43msuper\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m.\u001b[49m\u001b[43mlearn\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    451\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtotal_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    452\u001b[39m \u001b[43m        \u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m=\u001b[49m\u001b[43mcallback\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    453\u001b[39m \u001b[43m        \u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m=\u001b[49m\u001b[43mlog_interval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    454\u001b[39m \u001b[43m        \u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m=\u001b[49m\u001b[43mtb_log_name\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    455\u001b[39m \u001b[43m        \u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m=\u001b[49m\u001b[43mreset_num_timesteps\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    456\u001b[39m \u001b[43m        \u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m=\u001b[49m\u001b[43mprogress_bar\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    457\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\stable_baselines3\\common\\on_policy_algorithm.py:337\u001b[39m, in \u001b[36mOnPolicyAlgorithm.learn\u001b[39m\u001b[34m(self, total_timesteps, callback, log_interval, tb_log_name, reset_num_timesteps, progress_bar)\u001b[39m\n\u001b[32m    334\u001b[39m         \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m.ep_info_buffer \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[32m    335\u001b[39m         \u001b[38;5;28mself\u001b[39m.dump_logs(iteration)\n\u001b[32m--> \u001b[39m\u001b[32m337\u001b[39m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[43m.\u001b[49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    339\u001b[39m callback.on_training_end()\n\u001b[32m    341\u001b[39m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\sb3_contrib\\ppo_recurrent\\ppo_recurrent.py:414\u001b[39m, in \u001b[36mRecurrentPPO.train\u001b[39m\u001b[34m(self)\u001b[39m\n\u001b[32m    412\u001b[39m \u001b[38;5;66;03m# Optimization step\u001b[39;00m\n\u001b[32m    413\u001b[39m \u001b[38;5;28mself\u001b[39m.policy.optimizer.zero_grad()\n\u001b[32m--> \u001b[39m\u001b[32m414\u001b[39m \u001b[43mloss\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[32m    415\u001b[39m \u001b[38;5;66;03m# Clip grad norm\u001b[39;00m\n\u001b[32m    416\u001b[39m th.nn.utils.clip_grad_norm_(\u001b[38;5;28mself\u001b[39m.policy.parameters(), \u001b[38;5;28mself\u001b[39m.max_grad_norm)\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\torch\\_tensor.py:647\u001b[39m, in \u001b[36mTensor.backward\u001b[39m\u001b[34m(self, gradient, retain_graph, create_graph, inputs)\u001b[39m\n\u001b[32m    637\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[32m    638\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[32m    639\u001b[39m         Tensor.backward,\n\u001b[32m    640\u001b[39m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[32m   (...)\u001b[39m\u001b[32m    645\u001b[39m         inputs=inputs,\n\u001b[32m    646\u001b[39m     )\n\u001b[32m--> \u001b[39m\u001b[32m647\u001b[39m \u001b[43mtorch\u001b[49m\u001b[43m.\u001b[49m\u001b[43mautograd\u001b[49m\u001b[43m.\u001b[49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    648\u001b[39m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m=\u001b[49m\u001b[43minputs\u001b[49m\n\u001b[32m    649\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\torch\\autograd\\__init__.py:354\u001b[39m, in \u001b[36mbackward\u001b[39m\u001b[34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[39m\n\u001b[32m    349\u001b[39m     retain_graph = create_graph\n\u001b[32m    351\u001b[39m \u001b[38;5;66;03m# The reason we repeat the same comment below is that\u001b[39;00m\n\u001b[32m    352\u001b[39m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[32m    353\u001b[39m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[32m--> \u001b[39m\u001b[32m354\u001b[39m \u001b[43m_engine_run_backward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[32m    355\u001b[39m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    356\u001b[39m \u001b[43m    \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    357\u001b[39m \u001b[43m    \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    358\u001b[39m \u001b[43m    \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    359\u001b[39m \u001b[43m    \u001b[49m\u001b[43minputs_tuple\u001b[49m\u001b[43m,\u001b[49m\n\u001b[32m    360\u001b[39m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    361\u001b[39m \u001b[43m    \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[43m=\u001b[49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\n\u001b[32m    362\u001b[39m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[36mFile \u001b[39m\u001b[32mc:\\Users\\mohda\\miniconda3\\envs\\rl_a3\\Lib\\site-packages\\torch\\autograd\\graph.py:829\u001b[39m, in \u001b[36m_engine_run_backward\u001b[39m\u001b[34m(t_outputs, *args, **kwargs)\u001b[39m\n\u001b[32m    827\u001b[39m     unregister_hooks = _register_logging_hooks_on_whole_graph(t_outputs)\n\u001b[32m    828\u001b[39m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[32m--> \u001b[39m\u001b[32m829\u001b[39m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mVariable\u001b[49m\u001b[43m.\u001b[49m\u001b[43m_execution_engine\u001b[49m\u001b[43m.\u001b[49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[32m    830\u001b[39m \u001b[43m        \u001b[49m\u001b[43mt_outputs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m*\u001b[49m\u001b[43m*\u001b[49m\u001b[43mkwargs\u001b[49m\n\u001b[32m    831\u001b[39m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m  \u001b[38;5;66;03m# Calls into the C++ engine to run the backward pass\u001b[39;00m\n\u001b[32m    832\u001b[39m \u001b[38;5;28;01mfinally\u001b[39;00m:\n\u001b[32m    833\u001b[39m     \u001b[38;5;28;01mif\u001b[39;00m attach_logging_hooks:\n",
      "\u001b[31mKeyboardInterrupt\u001b[39m: "
     ]
    }
   ],
   "source": [
    "print(\"Starting PPO training with Stable Baselines3...\")\n",
    "model.learn(\n",
    "    total_timesteps=config['n_timesteps'], # 10 Million\n",
    "    callback=combined_callbacks\n",
    ")\n",
    "print(\"PPO Training Finished.\")\n",
    "print(f\"Total timesteps trained: {model.num_timesteps}\")\n",
    "\n",
    "# Evaluate the trained model\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "print(f\"This is the avg reward: {mean_reward:.2f} +/- {std_reward:.2f}\")\n",
    "\n",
    "# 5. Save the final model\n",
    "model.save(os.path.join(MODEL_DIR, f\"ppo_plantos_final_model-{training_run}\"))\n",
    "\n",
    "env.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b641990",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "rl_a3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
